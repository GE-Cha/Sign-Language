# Sign-Language

This project is made on behalf of a term project of Purdue CNIT 58100 Assistive Technology and Robotics (Fall 2018), which is a Deep Learning-based mobile application of sign language learning.

## Video
[![Project Video](https://i.ytimg.com/vi/5fNDHihrI6Q/hqdefault.jpg?sqp=-oaymwEZCNACELwBSFXyq4qpAwsIARUAAIhCGAFwAQ==&rs=AOn4CLCDmnM96RQw7-W-SQNWxIVBGeMogQ)](https://www.youtube.com/watch?v=5fNDHihrI6Q)

## Blog for the project
https://activesignlanguage.wordpress.com

## Model (Not updated)
* The model is trained by un-normalized American Sign Language (ASL) images captured by iPhone XS. The images which have been used for training are A, B, C, and H right-handed sign language pictures. As a structure of model, Very Deep Convolutional Networks[(VGG)](https://arxiv.org/abs/1409.1556) is used for further update. Since the training is based on Keras 2.2.4, the trained model has a different extension, therefore, a step of model conversion is required. A coremltools aid this step for using the trained model in the iOS environment. 

## UI
At first design step, this project is mainly about a DL based mobile application from individual idea. However, updating the application for educational purpose, the program is expanded to ASL learning and practice. 

* Initial Version
![](https://github.com/chagom/Sign-Language/blob/master/initial_design.png)

* Updated Version
![](https://github.com/chagom/Sign-Language/blob/master/updated_mobile_design.png)

## Education
In this application, users are expected to beginners of ASL; therefore, contents of this application are mainly focused on alphabets and numbers.

## Contribution


## Reference
** Description for alphabets from [here](http://www.deafblind.com/asl.html)
