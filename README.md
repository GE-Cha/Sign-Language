# Sign-Language

This project is made on behalf of a term project of Purdue CNIT 58100 Assistive Technology and Robotics (Fall 2018), which is a Deep Learning-based mobile application of sign language learning.

## Video
[![Project Video](https://i.ytimg.com/vi/5fNDHihrI6Q/hqdefault.jpg?sqp=-oaymwEZCNACELwBSFXyq4qpAwsIARUAAIhCGAFwAQ==&rs=AOn4CLCDmnM96RQw7-W-SQNWxIVBGeMogQ)](https://www.youtube.com/watch?v=5fNDHihrI6Q)

## Model (Not updated)
* The model is trained by un-normalized American Sign Language (ASL) images captured by iPhone XS. The images which have been used for training are A, B, C, and H right-handed sign language pictures. As a structure of model, Very Deep Convolutional Networks[(VGG)](https://arxiv.org/abs/1409.1556) is used for further update. Since the training is based on Keras 2.2.4, the trained model has a different extension, therefore, a step of model conversion is required. A coremltools aid this step for using the trained model in the iOS environment. 

## UI


## Education



## Contribution
